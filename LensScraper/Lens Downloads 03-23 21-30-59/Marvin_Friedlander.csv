"#",Jurisdiction,Kind,Display Key,Lens ID,Publication Date,Publication Year,Application Number,Application Date,Priority Numbers,Earliest Priority Date,Title,Abstract,Applicants,Inventors,Owners,URL,Document Type,Has Full Text,Cites Patent Count,Cited by Patent Count,Simple Family Size,Extended Family Size,Sequence Count,CPC Classifications,IPCR Classifications,US Classifications,NPL Citation Count,NPL Resolved Citation Count,NPL Resolved Lens ID(s),NPL Resolved External ID(s),NPL Citations,Legal Status
1,CN,A,CN 105359060 A,138-219-130-403-782,2016-02-24,2016,CN 201480031889 A,2014-06-03,US 2014/0040598 W;;US 201313909227 A,2013-06-04,Configuring user interface (UI) based on context,"A mobile apparatus determines whether it is disposed in a vehicle and, based on a determining that it is in the vehicle, automatically presents a first user interface (UI) that is simplified and/or easier to manipulate while driving relative to a second UI that would otherwise be presented.",SONY CORP,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,,https://lens.org/138-219-130-403-782,Patent Application,no,5,0,12,12,0,H04W4/027;;H04M2250/12;;H04M2250/22;;H04W4/50;;H04W4/029;;H04W4/18;;H04M1/72448;;H04W4/48;;H04W4/027;;H04M2250/22;;H04M2250/12;;H04W4/029;;H04W4/50;;H04M1/72448;;H04W4/18,G06F3/00;;H04W4/029;;H04W4/48;;H04W4/50,,0,0,,,,INACTIVE
2,CN,A,CN 104210441 A,189-471-189-672-447,2014-12-17,2014,CN 201410206762 A,2014-05-16,US 201313904253 A,2013-05-29,METHOD AND APPARATUS FOR AUTOMATICALLY WARNING DRIVER TO TAKE A BREAK,"A driver caution threshold that varies by time of day and/or by driver activity load factor is compared a vehicle driver's activity. Responsive to a determination that vehicle driver's activity violates the driver caution threshold, a prompt to take a break and/or directions to a nearby restaurant are presented.",SONY CORP,DAVID GEORGE THIELE;;STEVEN FRIEDLANDER;;MARVIN DEMERCHANT,,https://lens.org/189-471-189-672-447,Patent Application,no,5,3,2,2,0,B60W50/14;;B60W2040/0827;;B60W2050/146;;G01C21/3679;;G01C21/3679;;G01C21/3697;;G01C21/3697,B60R16/02;;H04L29/08,,0,0,,,,DISCONTINUED
3,WO,A1,WO 2014/197418 A1,178-468-163-152-682,2014-12-11,2014,US 2014/0040598 W,2014-06-03,US 201313909227 A,2013-06-04,CONFIGURING USER INTERFACE (UI) BASED ON CONTEXT,"A mobile apparatus determines whether it is disposed in a vehicle and, based on a determining that it is in the vehicle, automatically presents a first user interface (UI) that is simplified and/or easier to manipulate while driving relative to a second UI that would otherwise be presented.",SONY CORP;;THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,,https://lens.org/178-468-163-152-682,Patent Application,yes,7,1,12,12,0,H04W4/027;;H04M2250/12;;H04M2250/22;;H04W4/50;;H04W4/029;;H04W4/18;;H04M1/72448;;H04W4/48;;H04W4/027;;H04M2250/22;;H04M2250/12;;H04W4/029;;H04W4/50;;H04M1/72448;;H04W4/18,G06F3/00;;H04W4/029;;H04W4/48;;H04W4/50,,1,0,,,See also references of EP 2992401A4,PENDING
4,EP,A1,EP 2992401 A1,025-457-580-831-588,2016-03-09,2016,EP 14807609 A,2014-06-03,US 201313909227 A;;US 2014/0040598 W,2013-06-04,CONFIGURING USER INTERFACE (UI) BASED ON CONTEXT,,SONY CORP,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,SATURN LICENSING LLC (2017-12-13),https://lens.org/025-457-580-831-588,Patent Application,yes,0,0,12,12,0,H04W4/027;;H04M2250/12;;H04M2250/22;;H04W4/50;;H04W4/029;;H04W4/18;;H04M1/72448;;H04W4/48;;H04W4/027;;H04M2250/22;;H04M2250/12;;H04W4/029;;H04W4/50;;H04M1/72448;;H04W4/18,G06F3/00;;H04W4/029;;H04W4/48;;H04W4/50,,0,0,,,,DISCONTINUED
5,CA,C,CA 2914200 C,058-148-351-613-155,2019-01-15,2019,CA 2914200 A,2014-06-03,US 201313909227 A;;US 2014/0040598 W,2013-06-04,CONFIGURING USER INTERFACE (UI) BASED ON CONTEXT,"A mobile apparatus determines whether it is disposed in a vehicle and, based on a determining that it is in the vehicle, automatically presents a first user interface (UI) that is simplified and/or easier to manipulate while driving relative to a second UI that would otherwise be presented.",SONY CORP,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,,https://lens.org/058-148-351-613-155,Granted Patent,no,0,0,12,12,0,H04W4/027;;H04M2250/12;;H04M2250/22;;H04W4/50;;H04W4/029;;H04W4/18;;H04M1/72448;;H04W4/48;;H04W4/027;;H04M2250/22;;H04M2250/12;;H04W4/029;;H04W4/50;;H04M1/72448;;H04W4/18,G06F3/0481;;H04W4/029;;H04W4/40;;H04W4/48;;H04W4/50,,0,0,,,,ACTIVE
6,US,B2,US 9615231 B2,146-179-560-431-157,2017-04-04,2017,US 201313909227 A,2013-06-04,US 201313909227 A,2013-06-04,Configuring user interface (UI) based on context,"A mobile apparatus determines whether it is disposed in a vehicle and, based on a determining that it is in the vehicle, automatically presents a first user interface (UI) that is simplified and/or easier to manipulate while driving relative to a second UI that would otherwise be presented.",SONY CORP,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,SONY CORPORATION (2013-05-31);;SATURN LICENSING LLC (2019-01-08),https://lens.org/146-179-560-431-157,Granted Patent,yes,16,6,12,12,0,H04W4/027;;H04M2250/12;;H04M2250/22;;H04W4/50;;H04W4/029;;H04W4/18;;H04M1/72448;;H04W4/48;;H04W4/027;;H04M2250/22;;H04M2250/12;;H04W4/029;;H04W4/50;;H04M1/72448;;H04W4/18,G06F3/048;;G06F3/0481;;H04W4/029;;H04W4/18;;H04W4/48;;H04W4/50,,2,0,,,"Jussi Maaniitty, “Intelligent Mobile User Interface”, University of Tampere, School of Information Sciences Interactive Technology M.Sc. thesis, http://tutkielmat.uta.fi/pdf.gradu05494.pdf; Dec. 2011.;;Daivd Clegg, “Drive time with the HTC One X”, Telecom Tech, http://www.geekzone.co.nz/TelecomTech/8128; Jun. 8, 2012.",ACTIVE
7,EP,A4,EP 2992401 A4,153-474-434-391-516,2017-02-08,2017,EP 14807609 A,2014-06-03,US 201313909227 A;;US 2014/0040598 W,2013-06-04,CONFIGURING USER INTERFACE (UI) BASED ON CONTEXT,,SONY CORP,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,SATURN LICENSING LLC (2017-12-13),https://lens.org/153-474-434-391-516,Search Report,no,2,0,12,12,0,H04W4/027;;H04M2250/12;;H04M2250/22;;H04W4/50;;H04W4/029;;H04W4/18;;H04M1/72448;;H04W4/48;;H04W4/027;;H04M2250/22;;H04M2250/12;;H04W4/029;;H04W4/50;;H04M1/72448;;H04W4/18,G06F3/00;;H04W4/029;;H04W4/48;;H04W4/50,,1,0,,,See also references of WO 2014197418A1,DISCONTINUED
8,US,A1,US 2014/0358423 A1,117-594-140-221-686,2014-12-04,2014,US 201313904253 A,2013-05-29,US 201313904253 A,2013-05-29,METHOD AND APPARATUS FOR AUTOMATICALLY WARNING DRIVER TO TAKE A BREAK,"A driver caution threshold that varies by time of day and/or by driver activity load factor is compared a vehicle driver's activity. Responsive to a determination that vehicle driver's activity violates the driver caution threshold, a prompt to take a break and/or directions to a nearby restaurant are presented.",SONY CORP,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,SONY CORPORATION (2013-05-23),https://lens.org/117-594-140-221-686,Patent Application,yes,13,27,2,2,0,B60W50/14;;B60W2040/0827;;B60W2050/146;;G01C21/3679;;G01C21/3679;;G01C21/3697;;G01C21/3697,G01C21/36;;B60W50/14,701/423,0,0,,,,DISCONTINUED
9,KR,A,KR 20160003256 A,070-132-858-638-691,2016-01-08,2016,KR 20157034306 A,2014-06-03,US 201313909227 A;;US 2014/0040598 W,2013-06-04,CONFIGURING USER INTERFACE (UI) BASED ON CONTEXT,"모바일 디바이스는 그것이 차량 내에 배치되어 있는지 결정하고, 그것이 차량 내에 배치되어 있다는 결정에 기초하여, 다르게 제시될 제2 UI에 대해 상대적으로, 운전중인 동안 간단화된 그리고/또는 조작하기에 더 용이한 제1 사용자 인터페이스(UI)를 자동으로 제시한다.",SONY CORP,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,,https://lens.org/070-132-858-638-691,Patent Application,no,0,0,12,12,0,H04W4/027;;H04M2250/12;;H04M2250/22;;H04W4/50;;H04W4/029;;H04W4/18;;H04M1/72448;;H04W4/48;;H04W4/027;;H04M2250/22;;H04M2250/12;;H04W4/029;;H04W4/50;;H04M1/72448;;H04W4/18,H04W4/029;;H04W4/18;;H04W4/48;;H04W4/50,,0,0,,,,ACTIVE
10,CA,A1,CA 2914200 A1,012-067-821-424-903,2014-12-11,2014,CA 2914200 A,2014-06-03,US 201313909227 A;;US 2014/0040598 W,2013-06-04,CONFIGURING USER INTERFACE (UI) BASED ON CONTEXT,"A mobile apparatus determines whether it is disposed in a vehicle and, based on a determining that it is in the vehicle, automatically presents a first user interface (UI) that is simplified and/or easier to manipulate while driving relative to a second UI that would otherwise be presented.",SONY CORP,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,,https://lens.org/012-067-821-424-903,Patent Application,no,0,0,12,12,0,H04W4/027;;H04M2250/12;;H04M2250/22;;H04W4/50;;H04W4/029;;H04W4/18;;H04M1/72448;;H04W4/48;;H04W4/027;;H04M2250/22;;H04M2250/12;;H04W4/029;;H04W4/50;;H04M1/72448;;H04W4/18,G06F3/0481;;H04W4/029;;H04W4/48;;H04W4/50,,0,0,,,,ACTIVE
11,US,A1,US 2014/0359456 A1,184-583-401-839-692,2014-12-04,2014,US 201313909227 A,2013-06-04,US 201313909227 A,2013-06-04,CONFIGURING USER INTERFACE (UI) BASED ON CONTEXT,"A mobile apparatus determines whether it is disposed in a vehicle and, based on a determining that it is in the vehicle, automatically presents a first user interface (UI) that is simplified and/or easier to manipulate while driving relative to a second UI that would otherwise be presented.",SONY CORP,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN,SONY CORPORATION (2013-05-31);;SATURN LICENSING LLC (2019-01-08),https://lens.org/184-583-401-839-692,Patent Application,yes,3,36,12,12,0,H04W4/027;;H04M2250/12;;H04M2250/22;;H04W4/50;;H04W4/029;;H04W4/18;;H04M1/72448;;H04W4/48;;H04W4/027;;H04M2250/22;;H04M2250/12;;H04W4/029;;H04W4/50;;H04M1/72448;;H04W4/18,H04L12/24;;H04W4/029;;H04W4/48;;H04W4/50,715/735,0,0,,,,ACTIVE
12,CN,A,CN 104184984 A,092-123-003-824-898,2014-12-03,2014,CN 201410203639 A,2014-05-15,US 201313899781 A,2013-05-22,PORTABLE TRANSPARENT DISPLAY WITH LIFE-SIZE IMAGE FOR TELECONFERENCE,"A local teleconference participant can view a near-life-size image of a remote teleconference participant on a thin transparent upright display. Because the display is transparent, local background images that surround the image of the remote participant can be viewed through the display just as they would be if the remote participant were present locally.",SONY CORP,STEVEN FRIEDLANDER;;DAVID THIELE;;DAVID YOUNG;;MARVIN DEMERCHANT,,https://lens.org/092-123-003-824-898,Patent Application,no,4,2,4,4,0,H04N7/142;;H04N7/142,H04N7/15,,0,0,,,,DISCONTINUED
13,CN,A,CN 107046664 A,123-080-210-596-598,2017-08-15,2017,CN 201710066811 A,2017-02-07,US 201615018681 A,2016-02-08,AUTO-CONFIGURABLE SPEAKER SYSTEM,"Various aspects of a speaker system and a method for auto-configuration of the speaker system is disclosed herein. The speaker system includes an electronic device, which reproduce a first audio in a first speaker configuration. A first sound reproduction device is detected within a pre-defined range of the electronic device. Based on the detection, the first audio is communicated to the first sound reproduction device by the electronic device. The first speaker configuration is modified to a second speaker configuration to reproduce the communicated first audio at the first sound reproduction device.",SONY CORP,FRIEDLANDER STEVEN;;YOUNG DAVID;;YI HYEHOON;;DEMERCHANT MARVIN,,https://lens.org/123-080-210-596-598,Patent Application,no,4,2,8,8,0,H04R3/12;;H04R2430/00;;H04R3/12;;H04R5/02;;H04R5/04;;H04R2420/05;;H04R2420/07;;H04M1/6025;;H04M1/6033;;H04R5/02;;H04R2420/07;;H04R5/02;;H04R3/12;;H04R5/04;;H04R2420/05;;H04R2420/07,H04R3/12,,0,0,,,,ACTIVE
14,US,A1,US 2017/0230753 A1,126-426-667-894-848,2017-08-10,2017,US 201615018681 A,2016-02-08,US 201615018681 A,2016-02-08,AUTO-CONFIGURABLE SPEAKER SYSTEM,"Various aspects of a speaker system and a method for auto-configuration of the speaker system is disclosed herein. The speaker system includes an electronic device, which reproduce a first audio in a first speaker configuration. A first sound reproduction device is detected within a pre-defined range of the electronic device. Based on the detection, the first audio is communicated to the first sound reproduction device by the electronic device. The first speaker configuration is modified to a second speaker configuration to reproduce the communicated first audio at the first sound reproduction device.",SONY CORP,FRIEDLANDER STEVEN;;YOUNG DAVID;;YI HYEHOON;;DEMERCHANT MARVIN,SONY CORPORATION (2016-02-16),https://lens.org/126-426-667-894-848,Patent Application,yes,0,3,8,8,0,H04R3/12;;H04R2430/00;;H04R3/12;;H04R5/02;;H04R5/04;;H04R2420/05;;H04R2420/07;;H04M1/6025;;H04M1/6033;;H04R5/02;;H04R2420/07;;H04R5/02;;H04R3/12;;H04R5/04;;H04R2420/05;;H04R2420/07,H04R5/04,,0,0,,,,ACTIVE
15,JP,A,JP 2017143515 A,104-691-835-799-970,2017-08-17,2017,JP 2017020908 A,2017-02-08,US 201615018681 A,2016-02-08,AUTOMATICALLY CONFIGURABLE SPEAKER SYSTEM,"PROBLEM TO BE SOLVED: To provide a speaker system, especially an automatically configurable speaker system.SOLUTION: A speaker system includes an electronic device 102 reproducing a first audio in a first speaker configuration 110. A first sound reproduction device 104 is detected in a predetermined range of the electronic device. Based on this detection, the first audio is communicated by the electronic device to the first sound reproduction device. The first speaker configuration is corrected to a second speaker configuration 112 for reproducing the communicated first audio in the first sound reproduction device.SELECTED DRAWING: Figure 1",SONY CORP,STEVEN FRIEDLANDER;;DAVID YOUNG;;YI HYEHOON;;MARVIN DEMERCHANT,,https://lens.org/104-691-835-799-970,Patent Application,no,5,0,8,8,0,H04R3/12;;H04R2430/00;;H04R3/12;;H04R5/02;;H04R5/04;;H04R2420/05;;H04R2420/07;;H04M1/6025;;H04M1/6033;;H04R5/02;;H04R2420/07;;H04R5/02;;H04R3/12;;H04R5/04;;H04R2420/05;;H04R2420/07,H04S7/00;;H04R3/12,,0,0,,,,ACTIVE
16,US,B2,US 9866965 B2,154-041-277-063-254,2018-01-09,2018,US 201615018681 A,2016-02-08,US 201615018681 A,2016-02-08,Auto-configurable speaker system,"Various aspects of a speaker system and a method for auto-configuration of the speaker system is disclosed herein. The speaker system includes an electronic device, which reproduce a first audio in a first speaker configuration. A first sound reproduction device is detected within a pre-defined range of the electronic device. Based on the detection, the first audio is communicated to the first sound reproduction device by the electronic device. The first speaker configuration is modified to a second speaker configuration to reproduce the communicated first audio at the first sound reproduction device.",SONY CORP,FRIEDLANDER STEVEN;;YOUNG DAVID;;YI HYEHOON;;DEMERCHANT MARVIN,SONY CORPORATION (2016-02-16),https://lens.org/154-041-277-063-254,Granted Patent,yes,23,0,8,8,0,H04R3/12;;H04R2430/00;;H04R3/12;;H04R5/02;;H04R5/04;;H04R2420/05;;H04R2420/07;;H04M1/6025;;H04M1/6033;;H04R5/02;;H04R2420/07;;H04R5/02;;H04R3/12;;H04R5/04;;H04R2420/05;;H04R2420/07,H04R5/04,,1,0,,,"Office Action for KR Patent Application No. 10-2017-0015631, dated Aug. 28, 2017, 5 pages of Office Action and 3 pages of English Translation.",ACTIVE
17,WO,A3,WO 2014/186103 A3,086-156-360-390-251,2015-01-29,2015,US 2014/0035253 W,2014-04-24,US 201313893633 A,2013-05-14,AUTOMATIC FRIEND FOLLOWING APPLICATION,"Two friends can pair wireless communication devices which subsequently exchange positional information peer-to-peer or through a cloud server. Each device presents a map showing its location and the location of the paired device. Instructions can be provided to the device playing a follower role of how to reach the current location of the other device, which plays a leader role. In turn the leader device can present information as to the location of the follower device so that a user of the leader device can ascertain if the follower is off-track.",SONY CORP;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;YOUNG DAVID ANDREW;;THIELE DAVID,DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;YOUNG DAVID ANDREW;;THIELE DAVID,,https://lens.org/086-156-360-390-251,Search Report,yes,4,0,6,6,0,G06Q50/01;;G01C21/362;;H04M1/6075;;H04W84/18;;H04W4/21;;H04W4/80;;H04W4/02;;H04M1/72412;;H04M1/72457;;H04L67/52;;H04W4/21;;H04W4/80;;H04W4/029;;G06Q50/01;;G01C21/362;;H04M1/6075;;H04W84/18;;H04M1/72412;;H04M1/72457;;H04L67/52,G01C21/34;;H04M1/72412;;H04M1/72457,,0,0,,,,PENDING
18,US,B2,US 9100783 B2,153-768-020-192-49X,2015-08-04,2015,US 201313893633 A,2013-05-14,US 201313893633 A,2013-05-14,Automatic friend following application,"Two friends can pair wireless communication devices which subsequently exchange positional information peer-to-peer or through a cloud server. Each device presents a map showing its location and the location of the paired device. Instructions can be provided to the device playing a follower role of how to reach the current location of the other device, which plays a leader role. In turn the leader device can present information as to the location of the follower device so that a user of the leader device can ascertain if the follower is off-track.",SONY CORP,DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;YOUNG DAVID ANDREW;;THIELE DAVID,SONY CORPORATION (2013-05-13),https://lens.org/153-768-020-192-49X,Granted Patent,yes,8,2,6,6,0,G06Q50/01;;G01C21/362;;H04M1/6075;;H04W84/18;;H04W4/21;;H04W4/80;;H04W4/02;;H04M1/72412;;H04M1/72457;;H04L67/52;;H04W4/21;;H04W4/80;;H04W4/029;;G06Q50/01;;G01C21/362;;H04M1/6075;;H04W84/18;;H04M1/72412;;H04M1/72457;;H04L67/52,H04W4/02;;G01C21/36;;G06Q50/00;;H04L29/08;;H04M1/60;;H04M1/72412;;H04M1/72457;;H04W4/00;;H04W4/20;;H04W76/00,,2,2,038-658-408-896-50X;;047-820-921-913-795,10.1109/hicss.2002.994513;;10.1007/978-3-540-45233-1_12,"James Nord, Kare Synnes, Peter Parnes, ""An Architecture for Location Aware Applications"" Proceeding of the 35th Hawaii International Conference on System Sciences-2002, http://pure.ltu.se/portal.;;Rachel Fithian, Giovanni Iachelo, Jehan Moghazy, Zachary Pousman, John Stasko, ""The design and evaluation of a mobile location-aware handheld event planner"", College of computing/ GVU Center, Georgia Institute of Technology, 2003-http://pdf.aminer.org/000/495/127/the-design-and-evaluation-of-a-mobile-location-aware-handheld.pdf.",ACTIVE
19,CN,A,CN 105190246 A,050-060-316-175-930,2015-12-23,2015,CN 201480025402 A,2014-04-24,US 2014/0035253 W;;US 201313893633 A,2013-05-14,Automatic friend following application,"Two friends can pair wireless communication devices which subsequently exchange positional information peer-to-peer or through a cloud server. Each device presents a map showing its location and the location of the paired device. Instructions can be provided to the device playing a follower role of how to reach the current location of the other device, which plays a leader role. In turn the leader device can present information as to the location of the follower device so that a user of the leader device can ascertain if the follower is off-track.",SONY CORP,DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;YOUNG DAVID ANDREW;;THIELE DAVID,,https://lens.org/050-060-316-175-930,Patent Application,no,0,0,6,6,0,G06Q50/01;;G01C21/362;;H04M1/6075;;H04W84/18;;H04W4/21;;H04W4/80;;H04W4/02;;H04M1/72412;;H04M1/72457;;H04L67/52;;H04W4/21;;H04W4/80;;H04W4/029;;G06Q50/01;;G01C21/362;;H04M1/6075;;H04W84/18;;H04M1/72412;;H04M1/72457;;H04L67/52,G01C21/34;;H04M1/72412;;H04M1/72457,,0,0,,,,ACTIVE
20,US,A1,US 2014/0342753 A1,193-913-869-273-626,2014-11-20,2014,US 201313893633 A,2013-05-14,US 201313893633 A,2013-05-14,AUTOMATIC FRIEND FOLLOWING APPLICATION,"Two friends can pair wireless communication devices which subsequently exchange positional information peer-to-peer or through a cloud server. Each device presents a map showing its location and the location of the paired device. Instructions can be provided to the device playing a follower role of how to reach the current location of the other device, which plays a leader role. In turn the leader device can present information as to the location of the follower device so that a user of the leader device can ascertain if the follower is off-track.",SONY CORP,DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;YOUNG DAVID ANDREW;;THIELE DAVID,SONY CORPORATION (2013-05-13),https://lens.org/193-913-869-273-626,Patent Application,yes,2,12,6,6,0,G06Q50/01;;G01C21/362;;H04M1/6075;;H04W84/18;;H04W4/21;;H04W4/80;;H04W4/02;;H04M1/72412;;H04M1/72457;;H04L67/52;;H04W4/21;;H04W4/80;;H04W4/029;;G06Q50/01;;G01C21/362;;H04M1/6075;;H04W84/18;;H04M1/72412;;H04M1/72457;;H04L67/52,H04W4/02;;H04M1/72412;;H04M1/72457;;H04W4/00,455/456.2,0,0,,,,ACTIVE
21,WO,A2,WO 2014/186103 A2,040-345-403-495-365,2014-11-20,2014,US 2014/0035253 W,2014-04-24,US 201313893633 A,2013-05-14,AUTOMATIC FRIEND FOLLOWING APPLICATION,"Two friends can pair wireless communication devices which subsequently exchange positional information peer-to-peer or through a cloud server. Each device presents a map showing its location and the location of the paired device. Instructions can be provided to the device playing a follower role of how to reach the current location of the other device, which plays a leader role. In turn the leader device can present information as to the location of the follower device so that a user of the leader device can ascertain if the follower is off-track.",SONY CORP;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;YOUNG DAVID ANDREW;;THIELE DAVID,DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;YOUNG DAVID ANDREW;;THIELE DAVID,,https://lens.org/040-345-403-495-365,Patent Application,yes,0,0,6,6,0,G06Q50/01;;G01C21/362;;H04M1/6075;;H04W84/18;;H04W4/21;;H04W4/80;;H04W4/02;;H04M1/72412;;H04M1/72457;;H04L67/52;;H04W4/21;;H04W4/80;;H04W4/029;;G06Q50/01;;G01C21/362;;H04M1/6075;;H04W84/18;;H04M1/72412;;H04M1/72457;;H04L67/52,H04M1/72412;;H04M1/72457,,0,0,,,,PENDING
22,KR,A,KR 20170094080 A,055-906-716-287-833,2017-08-17,2017,KR 20170015631 A,2017-02-03,US 201615018681 A,2016-02-08,AUTO-CONFIGURABLE SPEAKER SYSTEM,"In the present disclosure, disclosed are a speaker system and a method for automatically configuring the same. The speaker system includes an electronic device playing a first audio in a first speaker configuration. A first sound playback device is detected within a predefined range of the electronic device. Based on the detection, the first audio is transmitted to the first sound playback device by the electronic device. The first speaker configuration is modified to a second speaker configuration in order to play the transmitted first audio in the first sound playback device.",SONY CORP,FRIEDLANDER STEVEN;;YOUNG DAVID;;YI HYE HOON;;DEMERCHANT MARVIN,,https://lens.org/055-906-716-287-833,Patent Application,no,0,2,8,8,0,H04R3/12;;H04R2430/00;;H04R3/12;;H04R5/02;;H04R5/04;;H04R2420/05;;H04R2420/07;;H04M1/6025;;H04M1/6033;;H04R5/02;;H04R2420/07;;H04R5/02;;H04R3/12;;H04R5/04;;H04R2420/05;;H04R2420/07,H04R5/02;;H04M1/60,,0,0,,,,ACTIVE
23,CN,A,CN 104217349 A,097-128-374-309-347,2014-12-17,2014,CN 201410240844 A,2014-06-03,US 201313909267 A,2013-06-04,SMART SHOPPING REMINDERS WHILE DRIVING,"A user of a wireless communication device (WCD) is reminded to stop at a store along a route of the user's vehicle based on a list of items needed, geographic location, and previous established behavior patterns, e.g., whether the user has stopped at a particular store close to the current time of day, along the current route of the vehicle, one or more times before. Shopping locations are verified at the location with a simple confirmation dialog on the WCD and suggested names are provided from a list of points of interests. Items to be purchased can then be added to a location-based shopping list by using a digital camera on the WCD to image the item and/or the barcode of the item and find the item in a database of images and barcodes.",SONY CORP,DAVID ANDREW YOUNG;;STEVEN FRIEDLANDER;;DAVID GEORGE THIELE;;MARVIN DEMERCHANT,,https://lens.org/097-128-374-309-347,Patent Application,no,5,2,2,2,0,G06Q30/0633;;G06Q30/0633;;G08G1/0962;;G08G1/0962;;H04M1/72457;;H04M1/72457;;H04W4/02;;H04W4/024;;H04W4/024;;H04W4/029;;H04W4/029,G06Q30/02,,0,0,,,,DISCONTINUED
24,JP,A,JP 2014230282 A,123-318-948-294-796,2014-12-08,2014,JP 2014104973 A,2014-05-21,US 201313899781 A,2013-05-22,PORTABLE TRANSPARENT DISPLAY WITH LIFE-SIZE IMAGE FOR TELECONFERENCE,"PROBLEM TO BE SOLVED: To provide a portable transparent display that realizes a feeling that a partner participant is actually in the room in teleconferencing.SOLUTION: Teleconferencing can feel unnatural when using a phone or tablet computer or even a TV, compared to an actual in-person dialog. A local teleconference participant can view a near-life-size image of a remote teleconference participant on a thin transparent upright display. Because the display is transparent, local background images that surround the image of the remote participant can be viewed through the display just as they would be if the remote participant were present locally.",SONY CORP,MARVIN DEMERCHANT;;DAVID ANDREW YOUNG;;STEVEN FRIEDLANDER;;DAVID GEORGE THIELE,,https://lens.org/123-318-948-294-796,Patent Application,no,4,2,4,4,0,H04N7/142;;H04N7/142,H04N7/15;;G09G3/20;;G09G5/00;;G09G5/36;;G09G5/377;;H04M3/56,,0,0,,,,PENDING
25,KR,A,KR 20140137302 A,127-874-428-870-086,2014-12-02,2014,KR 20140057703 A,2014-05-14,US 201313899781 A,2013-05-22,PORTABLE TRANSPARENT DISPLAY WITH LIFE-SIZE IMAGE FOR TELECONFERENCE,"A local video conference participant can see an almost life size image of a remote video conference participant on an upright type thin transparent display. Because the display is transparent, local background images surrounding the remote participant can be shown through the display just like the same background image which would be shown if the remote participant were present on the site.",SONY CORP,DEMERCHANT MARVIN;;YOUNG DAVID ANDREW;;FRIEDLANDER STEVEN;;THIELE DAVID GEORGE,,https://lens.org/127-874-428-870-086,Patent Application,no,0,0,4,4,0,H04N7/142;;H04N7/142,H04N7/15,,0,0,,,,DISCONTINUED
26,US,A1,US 2014/0358722 A1,155-288-956-412-04X,2014-12-04,2014,US 201313909267 A,2013-06-04,US 201313909267 A,2013-06-04,SMART SHOPPING REMINDERS WHILE DRIVING,"A user of a wireless communication device (WCD) is reminded to stop at a store along a route of the user's vehicle based on a list of items needed, geographic location, and previous established behavior patterns, e.g., whether the user has stopped at a particular store close to the current time of day, along the current route of the vehicle, one or more times before. Shopping locations are verified at the location with a simple confirmation dialog on the WCD and suggested names are provided from a list of points of interests. Items to be purchased can then be added to a location-based shopping list by using a digital camera on the WCD to image the item and/or the barcode of the item and find the item in a database of images and barcodes.",SONY CORP,THIELE DAVID GEORGE;;DEMERCHANT MARVIN;;YOUNG DAVID ANDREW;;FRIEDLANDER STEVEN,SONY CORPORATION (2013-05-30),https://lens.org/155-288-956-412-04X,Patent Application,yes,19,4,2,2,0,G06Q30/0633;;G06Q30/0633;;G08G1/0962;;G08G1/0962;;H04M1/72457;;H04M1/72457;;H04W4/02;;H04W4/024;;H04W4/024;;H04W4/029;;H04W4/029,G06Q30/06,705/26.8,1,0,,,"Garry, M., ""Cool Factor, The,"" Supermarket News, Vol. 58, No. 44, November 1, 2010",DISCONTINUED
27,KR,A,KR 20140135102 A,039-229-397-912-869,2014-11-25,2014,KR 20140055461 A,2014-05-09,US 201313893724 A,2013-05-14,METHOD AND APPARATUS FOR FINDING A LOST VEHICLE,"According to the present invention, when a driver leaves a vehicle in accordance to a command by a separation event such as a connection cutoff between a portable wireless communication device (WCD) of the driver and the vehicle through a Bluetooth, the WCD uploads the driver′s GPS position on a cloud server before the driver walks far away from the vehicle by a predetermined distance. The position is recorded, and then the driver may use the WCD to communicate with the server, thereby acquiring a map information which shows the present position of the WCD and the position recorded when the separation event occurs. Thus, the driver may recognize the relative position of the vehicle with respect to his present position.",SONY CORP,DEMERCHANT MARVIN;;YOUNG DAVID;;TAKAYA NORIFUMI;;THIELE DAVID;;FRIEDLANDER STEVEN,,https://lens.org/039-229-397-912-869,Patent Application,no,0,0,6,6,0,G01C21/00;;G01S5/0027;;G01S19/14;;G01C21/3423;;G01C21/3688;;G01S5/0027;;G07C5/008;;H04W4/02;;G01S5/0027;;G07C5/008;;H04W4/024;;G01S19/14;;G01C21/3423;;G01C21/3688,B60R25/10;;B60R25/00;;G06F15/16,,0,0,,,,ACTIVE
28,CN,A,CN 104154909 A,018-734-528-274-357,2014-11-19,2014,CN 201410195809 A,2014-05-12,US 201313893724 A,2013-05-14,Method and apparatus for finding a lost vehicle,"The invention relates to a method and apparatus for finding a lost vehicle. When a driver leaves a vehicle as indicated by a separation event such as the driver's portable wireless communication device (WCD) losing Bluetooth connectivity with the vehicle, the WCD uploads to a cloud server its GPS location before the driver has been able to walk away from the vehicle an appreciable distance. The location is recorded and the driver subsequently can use its WCD to communicate with the server to obtain map information showing the current location of the WCD and the location recorded at the separation event, so that the driver knows the location of her vehicle relative to her current location.",SONY CORP,MARVIN DEMERCHANT;;STEVEN FRIEDLANDER;;DAVID YOUNG;;DAVID THIELE;;NORIFUMI TAKAYA,,https://lens.org/018-734-528-274-357,Patent Application,no,6,1,6,6,0,G01C21/00;;G01S5/0027;;G01S19/14;;G01C21/3423;;G01C21/3688;;G01S5/0027;;G07C5/008;;H04W4/02;;G01S5/0027;;G07C5/008;;H04W4/024;;G01S19/14;;G01C21/3423;;G01C21/3688,G01C21/00,,0,0,,,,DISCONTINUED
29,US,A1,US 2014/0347436 A1,172-698-486-270-128,2014-11-27,2014,US 201313899781 A,2013-05-22,US 201313899781 A,2013-05-22,PORTABLE TRANSPARENT DISPLAY WITH LIFE-SIZE IMAGE FOR TELECONFERENCE,"A local teleconference participant can view a near-life-size image of a remote teleconference participant on a thin transparent upright display. Because the display is transparent, local background images that surround the image of the remote participant can be viewed through the display just as they would be if the remote participant were present locally.",SONY CORP,DEMERCHANT MARVIN;;YOUNG DAVID ANDREW;;FRIEDLANDER STEVEN;;THIELE DAVID GEORGE,SONY CORPORATION (2013-05-17),https://lens.org/172-698-486-270-128,Patent Application,yes,13,15,4,4,0,H04N7/142;;H04N7/142,H04N7/15,348/14.08,0,0,,,,DISCONTINUED
30,US,B2,US 8918272 B2,112-766-697-773-142,2014-12-23,2014,US 201313893724 A,2013-05-14,US 201313893724 A,2013-05-14,Method and apparatus for finding a lost vehicle,"When a driver leaves a vehicle as indicated by a separation event such as the driver's portable wireless communication device (WCD) losing Bluetooth connectivity with the vehicle, the WCD uploads to a cloud server its GPS location before the driver has been able to walk away from the vehicle an appreciable distance. The location is recorded and the driver subsequently can use its WCD to communicate with the server to obtain map information showing the current location of the WCD and the location recorded at the separation event, so that the driver knows the location of her vehicle relative to her current location.",SONY CORP,DEMERCHANT MARVIN;;YOUNG DAVID ANDREW;;TAKAYA NORIFUMI;;THIELE DAVID;;FRIEDLANDER STEVEN,SONY CORPORATION (2013-05-13),https://lens.org/112-766-697-773-142,Granted Patent,yes,7,3,6,6,0,G01C21/00;;G01S5/0027;;G01S19/14;;G01C21/3423;;G01C21/3688;;G01S5/0027;;G07C5/008;;H04W4/02;;G01S5/0027;;G07C5/008;;H04W4/024;;G01S19/14;;G01C21/3423;;G01C21/3688,G05D1/02;;G01C21/00;;G01S5/00;;G07C5/00;;H04W64/00,701/300;;701/31.4;;701/484;;340/438,1,0,,,"""Blue Tooth Locator"" web site printout http://www.bluetoothlocator.com/, printed Oct. 12, 2011.",ACTIVE
31,AU,A,AU 1978/038183 A,159-978-722-654-832,1980-01-24,1980,AU 1978/038183 A,1978-07-19,US 82185677 A;;US 83968677 A,1977-08-04,AMIDE ACRYLATE COMPOUNDS FOR USE IN RADIATION-CURABLE COATINGCOMPOSITIONS,,PPG INDUSTRIES INC,GRUBER GERALD WILLIAM;;FRIEDLANDER CHARLES BLAIR;;DOWBENKO ROSTYSLAW;;HUMKE BYRON MARVIN,,https://lens.org/159-978-722-654-832,Patent Application,no,0,1,13,16,0,C08F120/36;;C09D11/101,C08F2/00;;C07C233/07;;C07C233/18;;C07C233/25;;C07C233/65;;C07C233/68;;C07C233/69;;C07C235/06;;C07C235/08;;C07C235/16;;C08F2/48;;C08F20/00;;C08F20/34;;C08F120/36;;C09D11/10,,0,0,,,,EXPIRED
32,JP,A,JP 2014224815 A,135-669-473-593-636,2014-12-04,2014,JP 2014099274 A,2014-05-13,US 201313893724 A,2013-05-14,METHOD AND DEVICE FOR SEARCHING FOR LOST VEHICLE,"PROBLEM TO BE SOLVED: To find a vehicle in the case that a driver forgets where he has parked his vehicle.SOLUTION: When a driver moves away from his/her vehicle, which is indicated by a separation event such as a case where the driver's portable wireless communication device (WCD) loses Bluetooth connection with the vehicle, the WCD uploads its GPS position to a cloud server before the driver moves away from the vehicle more than a recognizable distance. The position is recorded, after which the driver communicates with the server using his/her WCD so that he/she will learn the position of the vehicle related to him/her relative to his/her current position, and can obtain map information indicating the current position of the wireless communication device and the position recorded by the separation event.",SONY CORP,MARVIN DEMERCHANT;;DAVID ANDREW YOUNG;;TAKAYA NORIFUMI;;DAVID THIELE;;STEVEN FRIEDLANDER,,https://lens.org/135-669-473-593-636,Patent Application,no,7,1,6,6,0,G01C21/00;;G01S5/0027;;G01S19/14;;G01C21/3423;;G01C21/3688;;G01S5/0027;;G07C5/008;;H04W4/02;;G01S5/0027;;G07C5/008;;H04W4/024;;G01S19/14;;G01C21/3423;;G01C21/3688,G01C21/26;;G01S19/46,,0,0,,,,PENDING
33,DE,A1,DE 2833825 A1,167-983-639-336-012,1979-02-08,1979,DE 2833825 A,1978-08-02,US 82185677 A;;US 83968677 A,1977-08-04,"ACRYLAMIDVERBINDUNGEN, VERFAHREN ZU IHRER HERSTELLUNG UND ZUSAMMENSETZUNGEN SOLCHER VERBINDUNGEN",,PPG INDUSTRIES INC,GRUBER GERALD WILLIAM;;DOWBENKO ROSTYSLAW;;FRIEDLANDER CHARLES BLAIR;;HUMKE BYRON MARVIN,,https://lens.org/167-983-639-336-012,Patent Application,no,0,0,13,16,0,C08F120/36;;C09D11/101,C08F2/00;;C07C233/07;;C07C233/18;;C07C233/25;;C07C233/65;;C07C233/68;;C07C233/69;;C07C235/06;;C07C235/08;;C07C235/16;;C08F2/48;;C08F20/00;;C08F20/34;;C08F120/36;;C09D11/10,,3,0,,,"Chemical Abstracts, Vol. 68, R. 60629a;;Chemical Abstracts, Vol. 72, R. 44480x;;Chemical Abstracts, Vol. 73, R. 60629a",EXPIRED
34,AU,B2,AU 523480 B2,001-898-576-690-674,1982-07-29,1982,AU 1978/038183 A,1978-07-19,US 82185677 A;;US 83968677 A,1977-08-04,AMIDE ACRYLATE COMPOUNDS FOR USE IN RADIATION-CURABLE COATINGCOMPOSITIONS,,PPG INDUSTRIES INC,GRUBER GERALD WILLIAM;;DOWBENKO ROSTYSLAW;;HUMKE BYRON MARVIN;;FRIEDLANDER CHARLES BLAIR,,https://lens.org/001-898-576-690-674,Granted Patent,no,0,0,13,16,0,C08F120/36;;C09D11/101,C08F2/00;;C07C233/07;;C07C233/18;;C07C233/25;;C07C233/65;;C07C233/68;;C07C233/69;;C07C235/06;;C07C235/08;;C07C235/16;;C08F2/48;;C08F20/00;;C08F20/34;;C08F120/36;;C09D11/10,,0,0,,,,EXPIRED
35,US,A1,US 2014/0343834 A1,150-380-750-893-087,2014-11-20,2014,US 201313893724 A,2013-05-14,US 201313893724 A,2013-05-14,METHOD AND APPARATUS FOR FINDING A LOST VEHICLE,"When a driver leaves a vehicle as indicated by a separation event such as the driver's portable wireless communication device (WCD) losing Bluetooth connectivity with the vehicle, the WCD uploads to a cloud server its GPS location before the driver has been able to walk away from the vehicle an appreciable distance. The location is recorded and the driver subsequently can use its WCD to communicate with the server to obtain map information showing the current location of the WCD and the location recorded at the separation event, so that the driver knows the location of her vehicle relative to her current location.",SONY CORP,DEMERCHANT MARVIN;;YOUNG DAVID ANDREW;;TAKAYA NORIFUMI;;THIELE DAVID;;FRIEDLANDER STEVEN,SONY CORPORATION (2013-05-13),https://lens.org/150-380-750-893-087,Patent Application,yes,2,88,6,6,0,G01C21/00;;G01S5/0027;;G01S19/14;;G01C21/3423;;G01C21/3688;;G01S5/0027;;G07C5/008;;H04W4/02;;G01S5/0027;;G07C5/008;;H04W4/024;;G01S19/14;;G01C21/3423;;G01C21/3688,G01C21/00,701/300,0,0,,,,ACTIVE
36,US,B2,US 10503373 B2,113-035-549-944-090,2019-12-10,2019,US 201213420275 A,2012-03-14,US 201213420275 A,2012-03-14,Visual feedback for highlight-driven gesture user interfaces,"Disclosed are approaches for user interface control. In one embodiment, a method can include: indicating a highlight of a first object of a plurality of objects displayed on a display screen; accepting a signal deriving from a first gesture input of a user indicating a neutral position relative to the highlighted first object; accepting a signal deriving from a second gesture input of the user indicating a directional position at least a predetermined distance from the neutral position; and indicating, in response to the second gesture input, a movement of the highlight from the first object to a second object adjacent to the first object, and where the directional position of the second gesture input correlates to a position on the display screen of the second object relative to the first object.",SCHWESIG CARSTEN;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;JOHNSON BRIAN;;YEH SEBRINA;;YI HYEHOON;;YOUNG DAVID;;SONY INTERACTIVE ENTERTAINMENT LLC,SCHWESIG CARSTEN;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;JOHNSON BRIAN;;YEH SEBRINA;;YI HYEHOON;;YOUNG DAVID,SONY INTERACTIVE ENTERTAINMENT LLC (2017-12-06);;SONY CORPORATION (2012-03-08);;SONY NETWORK ENTERTAINMENT INTERNATIONAL LLC (2012-03-08),https://lens.org/113-035-549-944-090,Granted Patent,yes,33,0,3,3,0,G06F3/04842;;G06F3/017;;G06F3/0304;;G06F3/0482,G06F3/0484;;G06F3/01;;G06F3/03;;G06F3/0482,,5,0,,,"Fabrizio Pilato Touchless Gesture User Interface to be Demo'd IFA Aug. 24, 2010 4 pages.;;C. Villamor, D. Willis, and I. Wroblewski Touch Gesture Reference Guide Apr. 15, 2010 7 pages.;;E. Artinger, T. Coskun, M. Schanzenbach, F. Echtler, S. Nestler, and G. Klinker. Exploring multi-touch gestures for map interaction in mass casualty incidents. In 3. Workshop zur IT-Unterstuetzung von Rettungskraeften im Rahmen der GI-Jahrestagung lnformatik 2011, Berlin, Germany, Oct. 2011.;;H. Choi, S. Chung, T. Crimm, J. Oh, M. Sencenbaugh CS247 Project 3: Gesture Controller Jan. 26, 2012 7 pages.;;Lin E; Cassidy A. et al.; “Hand Tracking using Spatial Gesture Modeling and Visual Feedback for a Virtual DJ System”; 2002; http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1166992.",ACTIVE
37,US,A1,US 2013/0246955 A1,067-712-792-259-404,2013-09-19,2013,US 201213420275 A,2012-03-14,US 201213420275 A,2012-03-14,VISUAL FEEDBACK FOR HIGHLIGHT-DRIVEN GESTURE USER INTERFACES,"Disclosed are approaches for user interface control. In one embodiment, a method can include: indicating a highlight of a first object of a plurality of objects displayed on a display screen; accepting a signal deriving from a first gesture input of a user indicating a neutral position relative to the highlighted first object; accepting a signal deriving from a second gesture input of the user indicating a directional position at least a predetermined distance from the neutral position; and indicating, in response to the second gesture input, a movement of the highlight from the first object to a second object adjacent to the first object, and where the directional position of the second gesture input correlates to a position on the display screen of the second object relative to the first object.",SCHWESIG CARSTEN;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;JOHNSON BRIAN;;YEH SABRINA;;YI HYEHOON;;YOUNG DAVID;;SONY NETWORK ENTERTAINMENT INT;;SONY CORP,SCHWESIG CARSTEN;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;JOHNSON BRIAN;;YEH SABRINA;;YI HYEHOON;;YOUNG DAVID,SONY INTERACTIVE ENTERTAINMENT LLC (2017-12-06);;SONY CORPORATION (2012-03-08);;SONY NETWORK ENTERTAINMENT INTERNATIONAL LLC (2012-03-08),https://lens.org/067-712-792-259-404,Patent Application,yes,35,66,3,3,0,G06F3/04842;;G06F3/017;;G06F3/0304;;G06F3/0482,G06F3/048;;G06F3/041,715/767,4,0,,,"Fabrizio Pilato Touchless Gesture User Interface to be Demo'd IFA 08/24/2010 4 pages;;C. Villamor, D. Willis, and l. Wroblewski Touch Gesture Reference Guide 04/15/2010 7 pages;;E. Artinger, T. Coskun, M. Schanzenbach, F. Echtler, S. Nestler, and G. Klinker. Exploring multi-touch gestures for map interaction in mass casualty incidents. In 3. Workshop zur IT-Unterstuetzung von Rettungskraeften im Rahmen der GI-Jahrestagung Informatik 2011, Berlin, Germany, October 2011;;H. Choi, S. Chung, T. Crimm, J. Oh, M. Sencenbaugh CS247 Project 3: Gesture Controller 01/26/2012 7 pages",ACTIVE
38,CN,A,CN 103309608 A,077-851-102-598-453,2013-09-18,2013,CN 201310068425 A,2013-03-05,US 201213420275 A,2012-03-14,Visual feedback for highlight-driven gesture user interfaces,"The invention relates to a visual feedback for highlight-driven gesture user interfaces and discloses an approaches for user interface control. In one embodiment, a method can include: indicating a highlight of a first object of a plurality of objects displayed on a display screen; accepting a signal deriving from a first gesture input of a user indicating a neutral position relative to the highlighted first object; accepting a signal deriving from a second gesture input of the user indicating a directional position at least a predetermined distance from the neutral position; and indicating, in response to the second gesture input, a movement of the highlight from the first object to a second object adjacent to the first object, and where the directional position of the second gesture input correlates to a position on the display screen of the second object relative to the first object.",SONY CORP;;SONY NETWORK ENTERTAINMENT INT,SCHWESIG CARSTEN;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;JOHNSON BRIAN;;YEH SABRINA;;YI HYEHOON;;YOUNG DAVID,,https://lens.org/077-851-102-598-453,Patent Application,no,3,3,3,3,0,G06F3/04842;;G06F3/017;;G06F3/0304;;G06F3/0482,G06F3/0488,,0,0,,,,DISCONTINUED
39,CA,A1,CA 2867147 A1,184-371-075-319-805,2013-10-03,2013,CA 2867147 A,2013-03-26,US 201213431638 A;;US 2013/0033774 W,2012-03-27,METHOD AND SYSTEM OF PROVIDING INTERACTIVE INFORMATION,"Some embodiments provide methods for use in providing information. These methods comprise: capturing, with one or more cameras of a display device, video along a first direction; detecting an object of interest that is captured in the video; obtaining additional information corresponding to the object of interest; determining an orientation of a user relative to a display of the display device, where the display is oriented opposite to the first direction; determining portions of each of the video images to be displayed based on the determined orientation of the user such that the portions of the video images are configured to appear to the user as though the display device were not positioned between the user and the object of interest; and displaying the portions of video images as they are captured and simultaneously displaying the additional information in cooperation with the object of interest.",SONY CORP,BAURMANN TRAVIS;;DAWSON THOMAS;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;HILL SETH;;YI HYEHOON;;YOUNG DAVID;;MILNE JAMES R,,https://lens.org/184-371-075-319-805,Patent Application,no,0,0,7,7,0,G06F3/14;;G06F3/0488;;G06F3/012;;G06F3/013;;G06F3/0488;;G06F3/14;;G06F3/012;;G06F3/013,G09G5/377,,0,0,,,,DISCONTINUED
40,CN,A,CN 104170003 A,104-985-054-041-327,2014-11-26,2014,CN 201380013791 A,2013-03-26,US 2013/0033774 W;;US 201213431638 A,2012-03-27,Method and system of providing interactive information,"Some embodiments provide methods for use in providing information. These methods comprise: capturing, with one or more cameras of a display device, video along a first direction; detecting an object of interest that is captured in the video; obtaining additional information corresponding to the object of interest; determining an orientation of a user relative to a display of the display device, where the display is oriented opposite to the first direction; determining portions of each of the video images to be displayed based on the determined orientation of the user such that the portions of the video images are configured to appear to the user as though the display device were not positioned between the user and the object of interest; and displaying the portions of video images as they are captured and simultaneously displaying the additional information in cooperation with the object of interest.",SONY CORP,BAURMANN TRAVIS;;DAWSON THOMAS;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;HILL SETH;;YI HYEHOON;;YOUNG DAVID;;MILNE JAMES R,,https://lens.org/104-985-054-041-327,Patent Application,no,3,7,7,7,0,G06F3/14;;G06F3/0488;;G06F3/012;;G06F3/013;;G06F3/0488;;G06F3/14;;G06F3/012;;G06F3/013,G09G5/00,,0,0,,,,DISCONTINUED
41,US,A1,US 2013/0260360 A1,015-730-636-167-092,2013-10-03,2013,US 201213431638 A,2012-03-27,US 201213431638 A,2012-03-27,METHOD AND SYSTEM OF PROVIDING INTERACTIVE INFORMATION,"Some embodiments provide methods for use in providing information. These methods comprise: capturing, with one or more cameras of a display device, video along a first direction; detecting an object of interest that is captured in the video; obtaining additional information corresponding to the object of interest; determining an orientation of a user relative to a display of the display device, where the display is oriented opposite to the first direction; determining portions of each of the video images to be displayed based on the determined orientation of the user such that the portions of the video images are configured to appear to the user as though the display device were not positioned between the user and the object of interest; and displaying the portions of video images as they are captured and simultaneously displaying the additional information in cooperation with the object of interest.",BAURMANN TRAVIS;;DAWSON THOMAS;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;HILL SETH;;YI HYEHOON;;YOUNG DAVID;;MILNE JAMES R;;SONY CORP,BAURMANN TRAVIS;;DAWSON THOMAS;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;HILL SETH;;YI HYEHOON;;YOUNG DAVID;;MILNE JAMES R,SONY CORPORATION (2012-03-20),https://lens.org/015-730-636-167-092,Patent Application,yes,5,52,7,7,0,G06F3/14;;G06F3/0488;;G06F3/012;;G06F3/013;;G06F3/0488;;G06F3/14;;G06F3/012;;G06F3/013,G09B25/00,434/365,0,0,,,,DISCONTINUED
42,WO,A1,WO 2013/148611 A1,097-072-285-963-726,2013-10-03,2013,US 2013/0033774 W,2013-03-26,US 201213431638 A,2012-03-27,METHOD AND SYSTEM OF PROVIDING INTERACTIVE INFORMATION,"Some embodiments provide methods for use in providing information. These methods comprise: capturing, with one or more cameras of a display device, video along a first direction; detecting an object of interest that is captured in the video; obtaining additional information corresponding to the object of interest; determining an orientation of a user relative to a display of the display device, where the display is oriented opposite to the first direction; determining portions of each of the video images to be displayed based on the determined orientation of the user such that the portions of the video images are configured to appear to the user as though the display device were not positioned between the user and the object of interest; and displaying the portions of video images as they are captured and simultaneously displaying the additional information in cooperation with the object of interest.",SONY CORP;;BAURMANN TRAVIS;;DAWSON THOMAS;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;HILL SETH;;YI HYEHOON;;YOUNG DAVID;;MILNE JAMES R,BAURMANN TRAVIS;;DAWSON THOMAS;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;HILL SETH;;YI HYEHOON;;YOUNG DAVID;;MILNE JAMES R,,https://lens.org/097-072-285-963-726,Patent Application,yes,5,2,7,7,0,G06F3/14;;G06F3/0488;;G06F3/012;;G06F3/013;;G06F3/0488;;G06F3/14;;G06F3/012;;G06F3/013,G09G5/00,,0,0,,,,PENDING
43,KR,A,KR 20140128428 A,150-225-850-850-439,2014-11-05,2014,KR 20147025485 A,2013-03-26,US 201213431638 A;;US 2013/0033774 W,2012-03-27,METHOD AND SYSTEM OF PROVIDING INTERACTIVE INFORMATION,,SONY CORP,BAURMANN TRAVIS;;DAWSON THOMAS;;DEMERCHANT MARVIN;;FRIEDLANDER STEVEN;;HILL SETH;;YI HYE HOON;;YOUNG DAVID;;MILNE JAMES R,,https://lens.org/150-225-850-850-439,Patent Application,no,0,0,7,7,0,G06F3/14;;G06F3/0488;;G06F3/012;;G06F3/013;;G06F3/0488;;G06F3/14;;G06F3/012;;G06F3/013,H04N21/43;;G06F3/01,,0,0,,,,DISCONTINUED
44,DE,C2,DE 2833825 C2,182-104-051-341-806,1984-01-19,1984,DE 2833825 A,1978-08-02,US 82185677 A;;US 83968677 A,1977-08-04,DE 2833825 C2,,"PPG INDUSTRIES, INC., 15222 PITTSBURGH, PA., US","GRUBER, GERALD WILLIAM, SEWICKLEY, PA., US;;DOWBENKO, ROSTYSLAW, GIBSONIA, PA., US;;FRIEDLANDER, CHARLES BLAIR;;HUMKE, BYRON MARVIN, GLENSHAW, PA., US",,https://lens.org/182-104-051-341-806,Granted Patent,no,0,0,13,16,0,C08F120/36;;C09D11/101,C08F2/00;;C07C233/07;;C07C233/18;;C07C233/25;;C07C233/65;;C07C233/68;;C07C233/69;;C07C235/06;;C07C235/08;;C07C235/16;;C08F2/48;;C08F20/00;;C08F20/34;;C08F120/36;;C09D11/10,,0,0,,,,EXPIRED
45,ZA,B,ZA 200905257 B,131-401-988-386-975,2010-10-27,2010,ZA 200905257 A,2008-02-08,US 88877707 P,2007-02-08,Behavioral recognition system,,BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,,https://lens.org/131-401-988-386-975,Granted Patent,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G06F/;;G08B/,,0,0,,,,ACTIVE
46,DK,T3,DK 2118864 T3,188-444-522-466-47X,2014-11-10,2014,DK 08729422 T,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,Adfærdsgenkendelsessystem,,BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;FRIEDLANDER DAVID SAMUEL;;XU GANG;;YANG TAO,,https://lens.org/188-444-522-466-47X,Granted Patent,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00;;G06K9/00;;G08B13/196;;G08B31/00,,0,0,,,,ACTIVE
47,EP,A4,EP 2118864 A4,005-278-925-859-676,2012-08-01,2012,EP 08729422 A,2008-02-08,US 2008/0053457 W;;US 88877707 P,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,,BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,"AVIGILON PATENT HOLDING 1 CORPORATION; CA (2021-10-29);;MOTOROLA SOLUTIONS, INC. (N. D. GES. D. STAATE, US (2022-09-21);;AVIGILON PATENT HOLDING 1 CORP., VANCOUVER, CA (2021-09-09);;MOTOROLA SOLUTIONS, INC.; US (2022-08-23);;9051147 CANADA INC., CA (2016-04-25);;9051147 CANADA INC.; CA (2016-01-01)",https://lens.org/005-278-925-859-676,Search Report,no,2,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00;;G06K9/00;;G08B13/196;;G08B31/00,,0,0,,,,ACTIVE
48,US,B2,US 8131012 B2,054-763-568-265-138,2012-03-06,2012,US 2848408 A,2008-02-08,US 2848408 A;;US 88877707 P,2007-02-08,Behavioral recognition system,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO;;BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,MOTOROLA SOLUTIONS INC (2022-04-11);;BEHAVIORAL RECOGNITION SYSTEMS INC (2008-02-07);;AVIGILON PATENT HOLDING 1 CORPORATION (2015-11-20),https://lens.org/054-763-568-265-138,Granted Patent,yes,50,51,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G06K9/00,382/103;;382/155;;340/573.1,13,9,081-306-760-591-422;;097-443-969-784-825;;043-932-892-009-14X;;014-072-551-477-851;;031-971-778-472-866;;019-147-449-556-436;;108-205-292-416-249;;009-517-087-800-034;;101-247-028-208-361,10.1109/cvpr.2007.383418;;10.1007/3-540-45053-x_48;;10.1109/34.868683;;10.1109/cvpr.1999.784637;;10.1109/34.868677;;10.1109/icme.2004.1394495;;10.1109/cvpr.2006.215;;10.1109/icpr.1998.711084;;10.1109/tpami.2004.110;;15521493,"PCT International Search Report & Written Opinion for PCT/US08/53457, dated Jul. 22, 2008.;;S. Apewokin et al., ""Multimodal Mean Adaptive Backgrounding for Embedded Real-Time Video Surveillance,"" Embedded Computer Vision Workshop (ECVW07) 2007: pp. 1-6.;;Ahmed Elgammal et al., ""Non-parametric model for background subtraction,"" IEEE Frame-Rate Workshop, 2000: pp. 751-767.;;Ismail Haritaoglu et al., ""W4: Real-Time Surveillance of People and Their Activities,"" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Aug. 2000, vol. 22(8): pp. 809-830.;;Yuri Ivanov et al., ""Video Surveillance of Interactions,"" CVPR Workshop on Visual Surveillance, 1998: pp. 82-89.;;Pentti Kanerva, ""Sparse Distributed Memory and Related Models,"" Associative Neural Memories: Theory and Implementation, New York: Oxford University Press, 1993: pp. 1-41.;;Andrew Senior et al., ""Appearance Models for Occlusion Handling,"" 2nd IEEE Workshop on Performance Evaluation of Tracking and Surveillance, 2001: pp. 1-8.;;Chris Stauffer et al., ""Adaptive background mixture models for real-time tracking,"" Proceedings IEEE Conference on Computer Vision and Pattern Recognition, 1999: pp. 246-252.;;Chris Stauffer et al., ""Learning Patterns of Activity Using Real-Time Tracking,"" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Aug. 2000, vol. 22(8): 747-757.;;J. Connell et al., ""Detection and Tracking in the IBM PeopleVision System,"" IEEE ICME, Jun. 2004: pp. 1-4, .;;Helmut Grabner et al., ""On-line Boosting and Vision,"" IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2006, vol. 1: pp. 260-267.;;Ismail Haritaoglu et al., ""Ghost: A Human Body Part Labeling System Using Silhouettes,"" 14th Annual International Conference on Pattern Recognition, Aug. 1998: pp. 77-82.;;Richard Nock et al., ""Statistical Region Merging,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, Nov. 2004, vol. 26(11): pp. 1452-1458.",ACTIVE
49,WO,A3,WO 2008/098188 A3,166-954-505-261-44X,2008-11-13,2008,US 2008/0053457 W,2008-02-08,US 88877707 P,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",BEHAVIORAL RECOGNITION SYSTEMS;;EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,,https://lens.org/166-954-505-261-44X,Search Report,yes,3,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00,,0,0,,,,PENDING
50,CN,A,CN 101622652 A,105-212-876-340-133,2010-01-06,2010,CN 200880004633 A,2008-02-08,US 2008/0053457 W;;US 88877707 P,2007-02-08,Behavioral recognition system,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",BEHAVIORAL RECOGNITION SYSTEMS,ERIC EATON JOHN;;KENNETH COBB WESLEY;;GENE URECH DENNIS;;ERNEST BLYTHE BOBBY;;SAMUEL FRIEDLANDER DAVID;;KUMAR GOTTUMUKKAL RAJKIRAN;;WILLIAM RISINGER LON;;ADINATH SAITWAL KISHOR;;MING-JUNG SEOW;;MARVIN SOLUM DAVID;;GANG XU;;TAO YANG,,https://lens.org/105-212-876-340-133,Patent Application,no,0,18,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00,,0,0,,,,ACTIVE
51,BR,A2,BR PI0806968 A2,011-959-762-539-731,2014-04-08,2014,BR PI0806968 A,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,SISTEMA DE RECONHECIMENTO COMPORTAMENTAL,,BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO;;RISINGER LON WILLIAM,,https://lens.org/011-959-762-539-731,Patent Application,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00,,0,0,,,,ACTIVE
52,US,B2,US 8620028 B2,121-896-494-958-296,2013-12-31,2013,US 201213413549 A,2012-03-06,US 201213413549 A;;US 2848408 A;;US 88877707 P,2007-02-08,Behavioral recognition system,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO;;BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,MOTOROLA SOLUTIONS INC (2022-04-11);;BEHAVIORAL RECOGNITION SYSTEMS INC (2008-02-07);;AVIGILON PATENT HOLDING 1 CORPORATION (2015-11-20),https://lens.org/121-896-494-958-296,Granted Patent,yes,89,16,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G06K9/62,382/103;;382/155;;382/224,19,8,019-147-449-556-436;;108-205-292-416-249;;009-517-087-800-034;;101-247-028-208-361;;081-306-760-591-422;;043-932-892-009-14X;;014-072-551-477-851;;031-971-778-472-866,10.1109/icme.2004.1394495;;10.1109/cvpr.2006.215;;10.1109/icpr.1998.711084;;10.1109/tpami.2004.110;;15521493;;10.1109/cvpr.2007.383418;;10.1109/34.868683;;10.1109/cvpr.1999.784637;;10.1109/34.868677,"EPO Supplementary European Search Report for EP 08729422 dated Jul. 3, 2012.;;J. Connell et al., ""Detection and Tracking in the IBM PeopleVision System,"" IEEE ICME, Jun. 2004: pp. 1-4, .;;Helmut Grabner et al., ""On-line Boosting and Vision,"" IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2006, vol. 1: pp. 260-267.;;Ismail Haritaoglu et al., ""Ghost: A Human Body Part Labeling System Using Silhouettes,"" 14th Annual International Conference on Pattern Recognition, Aug. 1998: pp. 77-82.;;Richard Nock et al., ""Statistical Region Merging,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, Nov. 2004, vol. 26(11): pp. 1452-1458.;;Apewokin et al. ""Multimodal Mean Adaptive Backgrounding for Embedded Real-Time Video Surveillance,"" Jun. 2007, IEEE 6 pages. Minneapolis, MN US.;;Elgammal et al. ""Non-parametric Model for Background Substraction,"" Computer Vision Laboratory, University of Maryland; Jun. 2000; 17 pages, College Park, MD US.;;Haritaogul et al. ""W4: Real-Time Surveillance of People and Their Activities,"" IEEE Transactions on Pattern Analysis and Machine Intelligence, Aug. 2000; vol. 22, No. 8; pp. 809-830.;;Ivanov et al. ""Video Surveillance of Interactions,"" MIT Media Laboratory, Cambridge, MA, Jul. 1999; 8 pages, Fort Collins, CO US.;;Chris Stauffer et al., ""Adaptive background mixture models for real-time tracking,"" Proceedings IEEE Conference on Computer Vision and Pattern Recognition, 1999: pp. 246-252.;;Pentti Kanerva ""Sparse Distributed memory and Related Models,"" M.H. Hassoun, ed., Associative Neural Memories: Theory and Implementation, 1993, pp. 50-76. New York: Oxford University Press.;;Senior et al. ""Appearance Models for Occlusion Handling,"" IBM T.J. Watson Research Center, 2001, 8 pages, Yorktown, Heights, NY US.;;Chris Stauffer et al., ""Learning Patterns of Activity Using Real-Time Tracking,"" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Aug. 2000, vol. 22(8): 747-757.;;PCT International Search Report and Written Opinion for PCT/US08/53457 dated Jul. 22, 2008.;;Fujiyoshi, H., VSAM: Video Surveillance and Monitoring System Project using Video Understanding Technology, Report of Information Processing Society of Japan, Information Processing Society of Japan, Nov. 9, 2001, vol. 2001, No. 106, pp. 67-74.;;Japanese Application No. 2009-549265 Office Action dated Dec. 4, 2012.;;Japanese Application No. 2012-132879 Office Action dated May 7, 2013.;;International Search Report Application No. PCT/US2004/033168 dated Apr. 3, 2005.;;Canadian Application No. 2,674,311 Office Action dated May 8, 2013.",ACTIVE
53,KR,A,KR 20090121309 A,106-637-244-905-394,2009-11-25,2009,KR 20097018650 A,2008-02-08,US 88877707 P,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",BEHAVIORAL RECOGNITION SYSTEMS,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,,https://lens.org/106-637-244-905-394,Patent Application,no,0,7,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00,,0,0,,,,ACTIVE
54,CA,C,CA 2674311 C,093-689-229-957-350,2015-12-29,2015,CA 2674311 A,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,,https://lens.org/093-689-229-957-350,Granted Patent,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G06T7/20;;G06F15/18;;G06K9/62;;G08B13/196;;H04N7/18,,0,0,,,,ACTIVE
55,US,A1,US 2012/0163670 A1,176-120-086-069-614,2012-06-28,2012,US 201213413549 A,2012-03-06,US 201213413549 A;;US 2848408 A;;US 88877707 P,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO;;BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,MOTOROLA SOLUTIONS INC (2022-04-11);;BEHAVIORAL RECOGNITION SYSTEMS INC (2008-02-07);;AVIGILON PATENT HOLDING 1 CORPORATION (2015-11-20),https://lens.org/176-120-086-069-614,Patent Application,yes,0,37,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G06K9/00,382/103,0,0,,,,ACTIVE
56,NZ,A,NZ 578752 A,055-493-826-104-329,2012-03-30,2012,NZ 57875208 A,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,"A method and a system for processing a stream of video frames recording events within a scene are disclosed. The system comprises: a video input source; a processor and a memory storing a computer vision engine and machine learning engine. The computer vision engine is configured to receive a first frame of the stream from the video input source. The first frame includes data for a plurality of pixels included in the frame. The computer vision engine is also configured to identify one or more groups of pixels in the first frame. Each group depicts an object (905, 910) within the scene. The computer vision engine is also configured to generate a search model storing one or more features associated with each identified object; classify each of the objects using a trained classifier; track, in a second frame, each of the objects identified in the first frame using the search model, and supply the first frame, the second frame, and the object classifications to a machine learning engine. The machine learning engine is configured to generate one or more semantic representations of behaviour engaged in by the objects in the scene over a plurality of frames and further configured to learn patterns of behaviour observed in the scene over the plurality of frames and to identify occurrences of the patterns of behaviour engaged in by the classified objects.",BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,"9051147 CANADA INC., CA (2016-02-25)",https://lens.org/055-493-826-104-329,Patent Application,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00;;G06K9/66;;G06N3/08,,0,0,,,,DISCONTINUED
57,ES,T3,ES 2522589 T3,063-290-026-527-85X,2014-11-17,2014,ES 08729422 T,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,Sistema de reconocimiento conductual,"Un procedimiento para procesar un flujo de tramas de vídeo que registra sucesos dentro de una escena, comprendiendo el procedimiento: recibir una primera trama del flujo (210, 215), en el que la primera trama incluye datos para una pluralidad de píxeles incluidos en la trama; identificar uno o más grupos de píxeles en la primera trama, en el que cada grupo representa un objeto dentro de la escena (225); generar un modelo de búsqueda que almacena una o más características asociadas con cada objeto identificado; clasificar cada uno de los objetos usando un clasificador entrenado (235); rastrear, en una segunda trama, cada uno de los objetos identificados en la primera trama usando el modelo de búsqueda (230); suministrar la primera trama, la segunda trama y las clasificaciones de objetos a un motor de aprendizaje automático; y generar, por el motor de aprendizaje automático, una o más representaciones semánticas de conducta en la que toman parte los objetos en la escena a lo largo de una pluralidad de tramas (245), en el que el motor de aprendizaje automático está configurado para aprender patrones de conducta observada en la escena a lo largo de la pluralidad de tramas (255) y para identificar apariciones de los patrones de conducta en la que toman parte los objetos clasificados (260).",BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,,https://lens.org/063-290-026-527-85X,Granted Patent,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00;;G06K9/00;;G08B13/196;;G08B31/00,,0,0,,,,ACTIVE
58,BR,A8,BR PI0806968 A8,139-991-122-770-056,2018-01-16,2018,BR PI0806968 A,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,sistema de reconhecimento comportamental,,BEHAVIORAL RECOGNITION SYS INC,BOBBY ERNEST BLYTHE;;DAVID MARVIN SOLUM;;DAVID SAMUEL FRIEDLANDER;;DENNIS GENE URECH;;GANG XU;;JOHN ERIC EATON;;KISHOR ADINATH SAITWAL;;LON WILLIAM RISINGER;;MING-JUNG SEOW;;RAJKIRAN KUMAR GOTTUMUKKAL;;TAO YANG;;WESLEY KENNETH COBB,,https://lens.org/139-991-122-770-056,Patent Application,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00,,0,0,,,,ACTIVE
59,CA,A1,CA 2674311 A1,086-285-882-104-022,2008-08-14,2008,CA 2674311 A,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,"Embodiments of the present invention provide a method and a system for an alyzing and learning behavior based on an acquired stream of video frames. O bjects depicted in the stream are determined based on an analysis of the vid eo frames. Each object may have a corresponding search model used to track a n object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic represen tations are used to determine objects' behaviors and to learn about behavior s occurring in an environment depicted by the acquired video streams. This w ay, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious beha vior based on what has been learned.",BEHAVIORAL RECOGNITION SYSTEMS,SOLUM DAVID MARVIN;;XU GANG;;YANG TAO;;EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;RISINGER LON WILLIAM;;SEOW MING-JUNG;;SAITWAL KISHOR ADINATH;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR,,https://lens.org/086-285-882-104-022,Patent Application,no,0,1,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00;;G06F15/18;;G06K9/62;;G08B13/196,,0,0,,,,ACTIVE
60,CN,B,CN 101622652 B,021-165-243-480-887,2012-03-21,2012,CN 200880004633 A,2008-02-08,US 2008/0053457 W;;US 88877707 P,2007-02-08,Behavioral recognition system,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",BEHAVIORAL RECOGNITION SYSTEMS US,ERIC EATON JOHN;;KENNETH COBB WESLEY;;GENE URECH DENNIS;;ERNEST BLYTHE BOBBY;;SAMUEL FRIEDLANDER DAVID;;KUMAR GOTTUMUKKAL RAJKIRAN;;WILLIAM RISINGER LON;;ADINATH SAITWAL KISHOR;;MING-JUNG SEOW;;MARVIN SOLUM DAVID;;GANG XU;;TAO YANG,,https://lens.org/021-165-243-480-887,Granted Patent,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00,,0,0,,,,ACTIVE
61,AU,A1,AU 2008/213586 A1,024-453-951-965-901,2008-08-14,2008,AU 2008/213586 A,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,Behavioral recognition system,,BEHAVIORAL RECOGNITION SYSTEMS,SEOW MING-JUNG;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;SOLUM DAVID MARVIN;;EATON JOHN ERIC;;SAITWAL KISHOR ADINATH;;URECH DENNIS GENE;;COBB WESLEY KENNETH;;RISINGER LON WILLIAM;;XU GANG;;YANG TAO,"MOTOROLA SOLUTIONS, INC. (2022-08-18);;9051147 CANADA INC. (2015-11-19);;AVIGILON PATENT HOLDING 1 CORPORATION (2021-09-02)",https://lens.org/024-453-951-965-901,Patent Application,no,0,3,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00,,0,0,,,,ACTIVE
62,EG,A,EG 26304 A,132-114-226-664-873,2013-07-03,2013,EG 2009081188 A,2009-08-06,US 2008/0053457 W;;US 88877707 P,2007-02-08,Behavioral recognition system and method within a video frame,,BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,,https://lens.org/132-114-226-664-873,Patent of Addition,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00,,0,0,,,,PENDING
63,WO,A2,WO 2008/098188 A2,164-326-935-049-190,2008-08-14,2008,US 2008/0053457 W,2008-02-08,US 88877707 P,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",BEHAVIORAL RECOGNITION SYSTEMS;;EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,,https://lens.org/164-326-935-049-190,Patent Application,yes,0,26,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G06F15/18,,0,0,,,,PENDING
64,JP,A,JP 2012230686 A,035-847-180-693-776,2012-11-22,2012,JP 2012132879 A,2012-06-12,US 88877707 P,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,"PROBLEM TO BE SOLVED: To provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames.SOLUTION: Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. In this way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such objects in the environment and identifies and predicts abnormal or suspicious behavior based on what has been learned.",BEHAVIORAL RECOGNITION SYS INC,JOHN ERIC EATON;;WESLEY KENNETH COBB;;DENNIS GENE URECH;;BOBBY ERNEST BLYTHE;;DAVID SAMUEL FRIEDLANDER;;RAJKIRAN KUMAR GOTTUMUKKAL;;LON WILLIAM RISINGER;;KISHOR ADINATH SAITWAL;;SEOW MING-JUNG;;DAVID MARVIN SOLUM;;XU GANG;;YANG TAO,,https://lens.org/035-847-180-693-776,Patent Application,no,6,1,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G06T7/00;;G01V8/10;;G06T7/20;;H04N7/18,,3,0,,,"JPN6013059971; 橋本 学: '安全・安心社会のための映像セキュリティ技術' システム／制御／情報 第５０巻 第１０号, 20061015, ｐ．１８-２３, システム制御情報学会;;CSNG200700151001; 橋本 学: '安全・安心社会のための映像セキュリティ技術' システム／制御／情報 第５０巻 第１０号, 20061015, ｐ．１８-２３, システム制御情報学会;;JPN6013059971; 橋本 学: '安全・安心社会のための映像セキュリティ技術' システム／制御／情報 第５０巻 第１０号, 20061015, ｐ．１８-２３, システム制御情報学会",ACTIVE
65,MX,A,MX 2009008376 A,179-272-403-257-726,2009-12-14,2009,MX 2009008376 A,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM.,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",BEHAVIORAL RECOGNITION SYSTEMS,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,,https://lens.org/179-272-403-257-726,Patent Application,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G06F15/18,,0,0,,,,ACTIVE
66,AU,B2,AU 2008/213586 B2,027-956-913-621-909,2013-07-11,2013,AU 2008/213586 A,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,Behavioral recognition system,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",MOTOROLA SOLUTIONS INC,RISINGER LON WILLIAM;;EATON JOHN ERIC;;BLYTHE BOBBY ERNEST;;SAITWAL KISHOR ADINATH;;YANG TAO;;SEOW MING-JUNG;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;COBB WESLEY KENNETH;;XU GANG;;URECH DENNIS GENE;;SOLUM DAVID MARVIN,"MOTOROLA SOLUTIONS, INC. (2022-08-18);;9051147 CANADA INC. (2015-11-19);;AVIGILON PATENT HOLDING 1 CORPORATION (2021-09-02)",https://lens.org/027-956-913-621-909,Granted Patent,no,1,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00,,0,0,,,,ACTIVE
67,EP,A2,EP 2118864 A2,093-397-568-001-439,2009-11-18,2009,EP 08729422 A,2008-02-08,US 2008/0053457 W;;US 88877707 P,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,,BEHAVIORAL RECOGNITION SYSTEMS,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,"AVIGILON PATENT HOLDING 1 CORPORATION; CA (2021-10-29);;MOTOROLA SOLUTIONS, INC. (N. D. GES. D. STAATE, US (2022-09-21);;AVIGILON PATENT HOLDING 1 CORP., VANCOUVER, CA (2021-09-09);;MOTOROLA SOLUTIONS, INC.; US (2022-08-23);;9051147 CANADA INC., CA (2016-04-25);;9051147 CANADA INC.; CA (2016-01-01)",https://lens.org/093-397-568-001-439,Patent Application,yes,0,1,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00;;G06K9/00;;G08B13/196;;G08B31/00,,0,0,,,,ACTIVE
68,EP,B1,EP 2118864 B1,045-226-378-901-682,2014-07-30,2014,EP 08729422 A,2008-02-08,US 2008/0053457 W;;US 88877707 P,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,,BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,"AVIGILON PATENT HOLDING 1 CORPORATION; CA (2021-10-29);;MOTOROLA SOLUTIONS, INC. (N. D. GES. D. STAATE, US (2022-09-21);;AVIGILON PATENT HOLDING 1 CORP., VANCOUVER, CA (2021-09-09);;MOTOROLA SOLUTIONS, INC.; US (2022-08-23);;9051147 CANADA INC., CA (2016-04-25);;9051147 CANADA INC.; CA (2016-01-01)",https://lens.org/045-226-378-901-682,Granted Patent,yes,5,1,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00;;G06K9/00;;G08B13/196;;G08B31/00,,0,0,,,,ACTIVE
69,US,A1,US 2008/0193010 A1,014-664-278-907-900,2008-08-14,2008,US 2848408 A,2008-02-08,US 2848408 A;;US 88877707 P,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,"Embodiments of the present invention provide a method and a system for analyzing and learning behavior based on an acquired stream of video frames. Objects depicted in the stream are determined based on an analysis of the video frames. Each object may have a corresponding search model used to track an object's motion frame-to-frame. Classes of the objects are determined and semantic representations of the objects are generated. The semantic representations are used to determine objects' behaviors and to learn about behaviors occurring in an environment depicted by the acquired video streams. This way, the system learns rapidly and in real-time normal and abnormal behaviors for any environment by analyzing movements or activities or absence of such in the environment and identifies and predicts abnormal and suspicious behavior based on what has been learned.",EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,MOTOROLA SOLUTIONS INC (2022-04-11);;BEHAVIORAL RECOGNITION SYSTEMS INC (2008-02-07);;AVIGILON PATENT HOLDING 1 CORPORATION (2015-11-20),https://lens.org/014-664-278-907-900,Patent Application,yes,20,198,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G06F15/18,382/159,0,0,,,,ACTIVE
70,BR,B1,BR PI0806968 B1,189-297-931-563-368,2018-09-18,2018,BR PI0806968 A,2008-02-08,US 88877707 P;;US 2008/0053457 W,2007-02-08,método para processar fluxo de quadros de vídeo e sistema associado,,BEHAVIORAL RECOGNITION SYS INC,BOBBY ERNEST BLYTHE;;DAVID MARVIN SOLUM;;DAVID SAMUEL FRIEDLANDER;;DENNIS GENE URECH;;GANG XU;;JOHN ERIC EATON;;KISHOR ADINATH SAITWAL;;LON WILLIAM RISINGER;;MING-JUNG SEOW;;RAJKIRAN KUMAR GOTTUMUKKAL;;TAO YANG;;WESLEY KENNETH COBB,,https://lens.org/189-297-931-563-368,Granted Patent,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00,,0,0,,,,ACTIVE
71,PL,T3,PL 2118864 T3,072-364-502-548-746,2015-03-31,2015,PL 08729422 T,2008-02-08,US 88877707 P;;EP 08729422 A;;US 2008/0053457 W,2007-02-08,BEHAVIORAL RECOGNITION SYSTEM,,BEHAVIORAL RECOGNITION SYS INC,EATON JOHN ERIC;;COBB WESLEY KENNETH;;URECH DENNIS GENE;;BLYTHE BOBBY ERNEST;;FRIEDLANDER DAVID SAMUEL;;GOTTUMUKKAL RAJKIRAN KUMAR;;RISINGER LON WILLIAM;;SAITWAL KISHOR ADINATH;;SEOW MING-JUNG;;SOLUM DAVID MARVIN;;XU GANG;;YANG TAO,,https://lens.org/072-364-502-548-746,Patent Application,no,0,0,33,33,0,G08B13/19608;;G08B13/19613;;G06V20/52;;G08B23/00;;G08B13/19608;;G08B13/19613;;G06V20/52,G08B23/00;;G06K9/00;;G08B13/196;;G08B31/00,,0,0,,,,PENDING
